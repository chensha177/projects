{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f79ce928",
      "metadata": {
        "id": "f79ce928"
      },
      "source": [
        "# CartPole DQN (PyTorch)\n",
        "\n",
        "Fixed epsilon-greedy + replay buffer + terminal masking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "99fb3a29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99fb3a29",
        "outputId": "0b016ecf-474d-41fa-ef17-7e28ea18063f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep   25 | train_return(last)=  15.0 | eval_mean(20)=  89.3 | eval_best(20)= 105.0 | eps=0.977 | buffer=589\n",
            "Ep   50 | train_return(last)=  26.0 | eval_mean(20)=  91.3 | eval_best(20)= 107.0 | eps=0.952 | buffer=1325\n",
            "Ep   75 | train_return(last)=  18.0 | eval_mean(20)=  92.2 | eval_best(20)= 108.0 | eps=0.927 | buffer=1950\n",
            "Ep  100 | train_return(last)=  37.0 | eval_mean(20)=  94.3 | eval_best(20)= 107.0 | eps=0.903 | buffer=2589\n",
            "Ep  125 | train_return(last)=  15.0 | eval_mean(20)=  92.1 | eval_best(20)= 104.0 | eps=0.879 | buffer=3187\n",
            "Ep  150 | train_return(last)=  31.0 | eval_mean(20)=  90.5 | eval_best(20)= 102.0 | eps=0.854 | buffer=3870\n",
            "Ep  175 | train_return(last)=  20.0 | eval_mean(20)=  92.7 | eval_best(20)= 106.0 | eps=0.829 | buffer=4556\n",
            "Ep  200 | train_return(last)=  27.0 | eval_mean(20)=  89.4 | eval_best(20)= 105.0 | eps=0.807 | buffer=5074\n",
            "Ep  225 | train_return(last)=  13.0 | eval_mean(20)=  90.0 | eval_best(20)= 107.0 | eps=0.783 | buffer=5738\n",
            "Ep  250 | train_return(last)=  33.0 | eval_mean(20)=  89.7 | eval_best(20)= 101.0 | eps=0.758 | buffer=6481\n",
            "Ep  275 | train_return(last)=  89.0 | eval_mean(20)=  92.3 | eval_best(20)= 105.0 | eps=0.730 | buffer=7445\n",
            "Ep  300 | train_return(last)=  26.0 | eval_mean(20)=  88.0 | eval_best(20)= 101.0 | eps=0.705 | buffer=8311\n",
            "Ep  325 | train_return(last)=  15.0 | eval_mean(20)=  89.2 | eval_best(20)= 104.0 | eps=0.677 | buffer=9325\n",
            "Ep  350 | train_return(last)=  15.0 | eval_mean(20)=  38.6 | eval_best(20)= 132.0 | eps=0.660 | buffer=10332\n",
            "Ep  375 | train_return(last)=  30.0 | eval_mean(20)= 121.4 | eval_best(20)= 290.0 | eps=0.626 | buffer=11302\n",
            "Ep  400 | train_return(last)=  47.0 | eval_mean(20)=  60.8 | eval_best(20)= 117.0 | eps=0.605 | buffer=12222\n",
            "Ep  425 | train_return(last)=  12.0 | eval_mean(20)=  29.8 | eval_best(20)= 113.0 | eps=0.591 | buffer=13075\n",
            "Ep  450 | train_return(last)=  14.0 | eval_mean(20)=   9.6 | eval_best(20)=  11.0 | eps=0.584 | buffer=13641\n",
            "Ep  475 | train_return(last)=  12.0 | eval_mean(20)=  17.0 | eval_best(20)=  28.0 | eps=0.577 | buffer=14029\n",
            "Ep  500 | train_return(last)=  25.0 | eval_mean(20)=  18.2 | eval_best(20)=  32.0 | eps=0.569 | buffer=14414\n",
            "Ep  525 | train_return(last)=  78.0 | eval_mean(20)= 275.6 | eval_best(20)= 367.0 | eps=0.504 | buffer=15532\n",
            "Ep  550 | train_return(last)= 327.0 | eval_mean(20)= 252.3 | eval_best(20)= 361.0 | eps=0.436 | buffer=17515\n",
            "Ep  575 | train_return(last)=  51.0 | eval_mean(20)= 249.1 | eval_best(20)= 351.0 | eps=0.354 | buffer=20884\n",
            "Ep  600 | train_return(last)= 265.0 | eval_mean(20)= 281.4 | eval_best(20)= 397.0 | eps=0.249 | buffer=25904\n",
            "Ep  625 | train_return(last)= 184.0 | eval_mean(20)= 303.2 | eval_best(20)= 407.0 | eps=0.131 | buffer=31953\n",
            "Ep  650 | train_return(last)= 145.0 | eval_mean(20)= 155.3 | eval_best(20)= 374.0 | eps=0.058 | buffer=36214\n",
            "Ep  675 | train_return(last)= 136.0 | eval_mean(20)= 140.0 | eval_best(20)= 153.0 | eps=0.020 | buffer=39781\n",
            "Ep  700 | train_return(last)= 135.0 | eval_mean(20)= 148.7 | eval_best(20)= 158.0 | eps=0.020 | buffer=43237\n",
            "Ep  725 | train_return(last)= 165.0 | eval_mean(20)= 139.3 | eval_best(20)= 163.0 | eps=0.020 | buffer=47001\n",
            "Ep  750 | train_return(last)= 154.0 | eval_mean(20)= 192.2 | eval_best(20)= 278.0 | eps=0.020 | buffer=50922\n",
            "Ep  775 | train_return(last)= 263.0 | eval_mean(20)= 253.6 | eval_best(20)= 348.0 | eps=0.020 | buffer=56919\n",
            "Ep  800 | train_return(last)= 185.0 | eval_mean(20)= 181.6 | eval_best(20)= 217.0 | eps=0.020 | buffer=63700\n",
            "Ep  825 | train_return(last)= 154.0 | eval_mean(20)= 269.9 | eval_best(20)= 400.0 | eps=0.020 | buffer=67859\n",
            "Ep  850 | train_return(last)= 171.0 | eval_mean(20)= 159.2 | eval_best(20)= 194.0 | eps=0.020 | buffer=72108\n",
            "Ep  875 | train_return(last)= 341.0 | eval_mean(20)= 225.2 | eval_best(20)= 401.0 | eps=0.020 | buffer=77346\n",
            "Ep  900 | train_return(last)= 240.0 | eval_mean(20)= 211.2 | eval_best(20)= 461.0 | eps=0.020 | buffer=82978\n",
            "Ep  925 | train_return(last)= 270.0 | eval_mean(20)= 196.3 | eval_best(20)= 233.0 | eps=0.020 | buffer=88144\n",
            "Ep  950 | train_return(last)= 247.0 | eval_mean(20)= 162.6 | eval_best(20)= 251.0 | eps=0.020 | buffer=94339\n",
            "Ep  975 | train_return(last)= 123.0 | eval_mean(20)= 127.3 | eval_best(20)= 132.0 | eps=0.020 | buffer=99325\n",
            "Ep 1000 | train_return(last)= 130.0 | eval_mean(20)= 120.7 | eval_best(20)= 125.0 | eps=0.020 | buffer=100000\n",
            "Ep 1025 | train_return(last)= 121.0 | eval_mean(20)= 154.8 | eval_best(20)= 265.0 | eps=0.020 | buffer=100000\n",
            "Ep 1050 | train_return(last)= 128.0 | eval_mean(20)= 130.6 | eval_best(20)= 135.0 | eps=0.020 | buffer=100000\n",
            "Ep 1075 | train_return(last)= 280.0 | eval_mean(20)= 194.0 | eval_best(20)= 340.0 | eps=0.020 | buffer=100000\n",
            "Ep 1100 | train_return(last)= 136.0 | eval_mean(20)= 289.1 | eval_best(20)= 500.0 | eps=0.020 | buffer=100000\n",
            "Ep 1125 | train_return(last)= 271.0 | eval_mean(20)= 263.9 | eval_best(20)= 354.0 | eps=0.020 | buffer=100000\n",
            "Ep 1150 | train_return(last)= 483.0 | eval_mean(20)= 162.6 | eval_best(20)= 354.0 | eps=0.020 | buffer=100000\n",
            "Ep 1175 | train_return(last)= 177.0 | eval_mean(20)= 134.6 | eval_best(20)= 157.0 | eps=0.020 | buffer=100000\n",
            "Ep 1200 | train_return(last)= 257.0 | eval_mean(20)= 165.8 | eval_best(20)= 313.0 | eps=0.020 | buffer=100000\n",
            "Ep 1225 | train_return(last)= 167.0 | eval_mean(20)= 396.4 | eval_best(20)= 500.0 | eps=0.020 | buffer=100000\n",
            "Ep 1250 | train_return(last)= 204.0 | eval_mean(20)= 124.0 | eval_best(20)= 148.0 | eps=0.020 | buffer=100000\n",
            "Ep 1275 | train_return(last)= 180.0 | eval_mean(20)= 195.2 | eval_best(20)= 217.0 | eps=0.020 | buffer=100000\n",
            "Ep 1300 | train_return(last)= 217.0 | eval_mean(20)= 192.4 | eval_best(20)= 214.0 | eps=0.020 | buffer=100000\n",
            "Ep 1325 | train_return(last)= 208.0 | eval_mean(20)= 252.4 | eval_best(20)= 428.0 | eps=0.020 | buffer=100000\n",
            "Ep 1350 | train_return(last)= 235.0 | eval_mean(20)= 299.2 | eval_best(20)= 364.0 | eps=0.020 | buffer=100000\n",
            "Ep 1375 | train_return(last)= 223.0 | eval_mean(20)= 198.2 | eval_best(20)= 224.0 | eps=0.020 | buffer=100000\n",
            "Ep 1400 | train_return(last)= 387.0 | eval_mean(20)= 132.8 | eval_best(20)= 153.0 | eps=0.020 | buffer=100000\n",
            "Ep 1425 | train_return(last)= 312.0 | eval_mean(20)= 217.1 | eval_best(20)= 500.0 | eps=0.020 | buffer=100000\n",
            "Ep 1450 | train_return(last)= 267.0 | eval_mean(20)= 262.8 | eval_best(20)= 333.0 | eps=0.020 | buffer=100000\n",
            "Ep 1475 | train_return(last)= 275.0 | eval_mean(20)= 313.0 | eval_best(20)= 405.0 | eps=0.020 | buffer=100000\n",
            "Ep 1500 | train_return(last)= 500.0 | eval_mean(20)= 500.0 | eval_best(20)= 500.0 | eps=0.020 | buffer=100000\n",
            "Solved (eval_mean >= 475).\n",
            "\n",
            "Final 100-episode mean return: 310.7\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "from typing import Deque, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Try gymnasium first, fall back to gym\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYMNASIUM = True\n",
        "except ImportError:\n",
        "    import gym\n",
        "    GYMNASIUM = False\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Utils\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def to_tensor(x, device):\n",
        "    return torch.as_tensor(x, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Replay Buffer\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class Transition:\n",
        "    s: np.ndarray\n",
        "    a: int\n",
        "    r: float\n",
        "    s2: np.ndarray\n",
        "    done: float  # 1.0 if terminal else 0.0\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buf: Deque[Transition] = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "    def push(self, s, a, r, s2, done: bool):\n",
        "        self.buf.append(Transition(s, int(a), float(r), s2, float(done)))\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buf, batch_size)\n",
        "        s  = np.stack([t.s  for t in batch], axis=0)\n",
        "        a  = np.array([t.a  for t in batch], dtype=np.int64)\n",
        "        r  = np.array([t.r  for t in batch], dtype=np.float32)\n",
        "        s2 = np.stack([t.s2 for t in batch], axis=0)\n",
        "        d  = np.array([t.done for t in batch], dtype=np.float32)\n",
        "        return s, a, r, s2, d\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Q Network\n",
        "# -------------------------\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, action_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# DQN Agent\n",
        "# -------------------------\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        device: torch.device,\n",
        "        gamma: float = 0.99,\n",
        "        lr: float = 5e-4,\n",
        "        batch_size: int = 64,\n",
        "        buffer_size: int = 50_000,\n",
        "        min_buffer: int = 2_000,\n",
        "        target_update_freq: int = 1_000,  # in env steps\n",
        "        eps_start: float = 1.0,\n",
        "        eps_end: float = 0.05,\n",
        "        eps_decay_steps: int = 50_000,\n",
        "        double_dqn: bool = True,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.min_buffer = min_buffer\n",
        "        self.target_update_freq = target_update_freq\n",
        "\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay_steps = eps_decay_steps\n",
        "        self.step = 0\n",
        "\n",
        "        self.double_dqn = double_dqn\n",
        "\n",
        "        self.q = QNet(state_dim, action_dim).to(device)\n",
        "        self.q_tgt = QNet(state_dim, action_dim).to(device)\n",
        "        self.q_tgt.load_state_dict(self.q.state_dict())\n",
        "        self.q_tgt.eval()\n",
        "\n",
        "        self.opt = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.SmoothL1Loss()  # Huber\n",
        "\n",
        "        self.rb = ReplayBuffer(buffer_size)\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "    def epsilon(self):\n",
        "        # linear decay\n",
        "        frac = min(1.0, self.step / float(self.eps_decay_steps))\n",
        "        return self.eps_start - (self.eps_start - self.eps_end) * frac\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, s, greedy: bool = False):\n",
        "        eps = 0.0 if greedy else self.epsilon()\n",
        "        self.step += 1\n",
        "\n",
        "        if random.random() < eps:\n",
        "            return random.randrange(self.action_dim)\n",
        "\n",
        "        s_t = to_tensor(s, self.device).unsqueeze(0)\n",
        "        qvals = self.q(s_t)\n",
        "        return int(torch.argmax(qvals, dim=1).item())\n",
        "\n",
        "    def push(self, s, a, r, s2, done: bool):\n",
        "        self.rb.push(s, a, r, s2, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.rb) < self.min_buffer:\n",
        "            return None\n",
        "\n",
        "        s, a, r, s2, d = self.rb.sample(self.batch_size)\n",
        "\n",
        "        s_t  = to_tensor(s, self.device)\n",
        "        s2_t = to_tensor(s2, self.device)\n",
        "        a_t  = torch.as_tensor(a, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
        "        r_t  = torch.as_tensor(r, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        d_t  = torch.as_tensor(d, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "\n",
        "        q_sa = self.q(s_t).gather(1, a_t)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.double_dqn:\n",
        "                # action selection from online net, evaluation from target net\n",
        "                a2 = torch.argmax(self.q(s2_t), dim=1, keepdim=True)\n",
        "                q_next = self.q_tgt(s2_t).gather(1, a2)\n",
        "            else:\n",
        "                q_next = self.q_tgt(s2_t).max(dim=1, keepdim=True)[0]\n",
        "\n",
        "            y = r_t + (1.0 - d_t) * self.gamma * q_next  # terminal masking\n",
        "\n",
        "        loss = self.loss_fn(q_sa, y)\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.q.parameters(), 10.0)\n",
        "        self.opt.step()\n",
        "\n",
        "        # target update\n",
        "        if self.step % self.target_update_freq == 0:\n",
        "            self.q_tgt.load_state_dict(self.q.state_dict())\n",
        "\n",
        "        return float(loss.item())\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Train / Eval\n",
        "# -------------------------\n",
        "def reset_env(env):\n",
        "    out = env.reset()\n",
        "    if isinstance(out, tuple):\n",
        "        obs = out[0]\n",
        "    else:\n",
        "        obs = out\n",
        "    return obs\n",
        "\n",
        "def step_env(env, action):\n",
        "    out = env.step(action)\n",
        "    if GYMNASIUM:\n",
        "        obs2, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "    else:\n",
        "        obs2, reward, done, info = out\n",
        "    return obs2, reward, done\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(env, agent: DQNAgent, episodes: int = 20):\n",
        "    scores = []\n",
        "    for _ in range(episodes):\n",
        "        s = reset_env(env)\n",
        "        done = False\n",
        "        total = 0.0\n",
        "        while not done:\n",
        "            a = agent.act(s, greedy=True)\n",
        "            s, r, done = step_env(env, a)\n",
        "            total += r\n",
        "        scores.append(total)\n",
        "    return float(np.mean(scores)), float(np.max(scores))\n",
        "\n",
        "\n",
        "def train_cartpole(\n",
        "    env_name: str = \"CartPole-v1\",\n",
        "    seed: int = 0,\n",
        "    episodes: int = 800,\n",
        "    eval_every: int = 25,\n",
        "):\n",
        "    set_seed(seed)\n",
        "    env = gym.make(env_name)\n",
        "    env_eval = gym.make(env_name)\n",
        "\n",
        "    # seed envs\n",
        "    try:\n",
        "        env.reset(seed=seed)\n",
        "        env_eval.reset(seed=seed + 1)\n",
        "    except TypeError:\n",
        "        pass\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # agent = DQNAgent(\n",
        "    #     state_dim=state_dim,\n",
        "    #     action_dim=action_dim,\n",
        "    #     device=device,\n",
        "    #     gamma=0.99,\n",
        "    #     lr=5e-4,\n",
        "    #     batch_size=64,\n",
        "    #     buffer_size=50_000,\n",
        "    #     min_buffer=2_000,\n",
        "    #     target_update_freq=1_000,\n",
        "    #     eps_start=1.0,\n",
        "    #     eps_end=0.05,\n",
        "    #     eps_decay_steps=50_000,\n",
        "    #     double_dqn=True,\n",
        "    # )\n",
        "    agent = DQNAgent(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    device=device,\n",
        "    gamma=0.99,\n",
        "    lr=1e-4,\n",
        "    batch_size=128,\n",
        "    buffer_size=100_000,\n",
        "    min_buffer=10_000,\n",
        "    target_update_freq=2000,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.02,\n",
        "    eps_decay_steps=100_000,\n",
        "    double_dqn=True,\n",
        ")\n",
        "\n",
        "    returns = []\n",
        "    losses = []\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        s = reset_env(env)\n",
        "        done = False\n",
        "        ep_ret = 0.0\n",
        "\n",
        "        while not done:\n",
        "            a = agent.act(s, greedy=False)\n",
        "            s2, r, done = step_env(env, a)\n",
        "            agent.push(s, a, r, s2, done)\n",
        "            s = s2\n",
        "            ep_ret += r\n",
        "\n",
        "            loss = agent.learn()\n",
        "            if loss is not None:\n",
        "                losses.append(loss)\n",
        "\n",
        "        returns.append(ep_ret)\n",
        "\n",
        "        if ep % eval_every == 0:\n",
        "            mean_eval, best_eval = evaluate(env_eval, agent, episodes=20)\n",
        "            print(f\"Ep {ep:4d} | train_return(last)={ep_ret:6.1f} | \"\n",
        "                  f\"eval_mean(20)={mean_eval:6.1f} | eval_best(20)={best_eval:6.1f} | \"\n",
        "                  f\"eps={agent.epsilon():.3f} | buffer={len(agent.rb)}\")\n",
        "\n",
        "            # Early stop heuristic for CartPole-v1\n",
        "            if mean_eval >= 475:\n",
        "                print(\"Solved (eval_mean >= 475).\")\n",
        "                break\n",
        "\n",
        "    env.close()\n",
        "    env_eval.close()\n",
        "    return agent, np.array(returns, dtype=float), np.array(losses, dtype=float)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent, returns, losses = train_cartpole(\n",
        "        env_name=\"CartPole-v1\",   # if you must use v0: \"CartPole-v0\"\n",
        "        seed=0,\n",
        "        episodes=1500,\n",
        "        eval_every=25,\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal 100-episode mean return:\", returns[-100:].mean() if len(returns) >= 100 else returns.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_eval = gym.make(\"CartPole-v1\")\n",
        "mean_100, best_100 = evaluate(env_eval, agent, episodes=100)\n",
        "print(f\"[FINAL TEST] mean(100)={mean_100:.1f}, best(100)={best_100:.1f}\")\n",
        "env_eval.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9jcOLeRN9WD",
        "outputId": "21e31cdd-16dd-4b1e-8969-9d1e987de33c"
      },
      "id": "I9jcOLeRN9WD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL TEST] mean(100)=500.0, best(100)=500.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}